# -*- coding: utf-8 -*-
"""streamlit_app.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10NNokAqvf8SiRfevbf707EO29ZrHMoBD
"""

# -*- coding: utf-8 -*-
"""
streamlit_app.py

Streamlit application for Healthcare Provider (HCP) Segmentation and
Sales Territory Optimization.

This script implements an interactive web application using Streamlit
for pharmaceutical sales operations. It allows users to:
1. Upload HCP data (CSV) and ZCTA boundary data (Shapefile in ZIP).
2. Perform K-Means clustering on ZCTA-aggregated data for segmentation based on
   user-selected features and cluster count.
3. Perform territory optimization using the SKATER algorithm on ZCTAs,
   aiming for a specified number of contiguous territories, balancing
   based on a selected ZCTA attribute, and respecting a minimum size constraint.
4. Visualize the optimized territories and optionally the HCP segments
   on an interactive map using Folium.
5. Display summary statistics and data previews for both segmentation
   and territory optimization results in separate tabs.
6. Allow users to download the optimized territory data (excluding geometry) as CSV.
"""

import streamlit as st
import pandas as pd
import geopandas as gpd
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer # Used for handling NaNs before scaling
import libpysal.weights
from spopt.region import Skater
import folium
from streamlit_folium import st_folium
import zipfile
import tempfile
import os
import io
import warnings
import matplotlib.pyplot as plt
import numpy as np
import matplotlib.cm as cm # Added for colormap generation
import matplotlib.colors as mcolors # Added for colormap generation
from shapely.geometry import Point
import shutil # For cleaning up temp dir

# --- Configuration ---

# Suppress specific warnings for cleaner output
warnings.filterwarnings('ignore', category=FutureWarning)
warnings.filterwarnings('ignore', category=UserWarning)
warnings.filterwarnings('ignore', message="The weights matrix is not fully connected") # Common with real-world data

# Target Coordinate Reference System (CRS) for all spatial data.
# Using WGS 84 (latitude/longitude) for Folium compatibility.
TARGET_CRS = 'EPSG:4326'

# Default column names (adjust if your input files use different names)
HCP_ID_COL = 'hcp_id' # Unique identifier for HCPs
HCP_LAT_COL = 'latitude'
HCP_LON_COL = 'longitude'
HCP_TRX_COL = 'trx_volume' # Example feature column
HCP_ZIP_COL = 'zip' # ZIP code in HCP file (lowercase for consistency)
# Column containing ZCTA codes in the shapefile (common TIGER/Line name)
# Will try common alternatives if this exact name isn't found.
ZCTA_COL_PRIMARY = 'ZCTA5CE20'

# Default map settings
DEFAULT_MAP_LOCATION = [39.8283, -98.5795] # Center of the US
DEFAULT_MAP_ZOOM = 4

# --- Helper Functions ---

def generate_colormap(num_colors):
    """Generates a discrete colormap for visualization."""
    # Using a qualitative map like 'tab20' for distinct categories
    if num_colors <= 10:
        cmap = cm.get_cmap('tab10', num_colors)
    elif num_colors <= 20:
        cmap = cm.get_cmap('tab20', num_colors)
    else:
        # Fallback for more colors using viridis, might not be visually distinct
        cmap = cm.get_cmap('viridis', num_colors)
    return [mcolors.to_hex(cmap(i)) for i in range(num_colors)]

@st.cache_data(ttl=3600) # Cache data loading for 1 hour
def load_data(hcp_upload_bytes, zcta_upload_bytes):
    """
    Loads HCP data from CSV bytes and ZCTA boundaries from ZIP bytes.
    Performs validation, cleaning, CRS transformation, and initial aggregation.

    Args:
        hcp_upload_bytes (bytes): Bytes content of the HCP CSV file.
        zcta_upload_bytes (bytes): Bytes content of the ZCTA ZIP file.

    Returns:
        tuple: (GeoDataFrame/DataFrame, GeoDataFrame) containing HCP data and ZCTA data,
               or (None, None) if loading fails. Returns ZCTA gdf with an
               'hcp_count' column added. HCP data is returned as GeoDataFrame
               if lat/lon are present, otherwise as DataFrame.
    """
    if not hcp_upload_bytes or not zcta_upload_bytes:
        return None, None

    hcp_data_out = None
    zcta_gdf = None
    temp_dir_obj = None # Use TemporaryDirectory for automatic cleanup

    with st.spinner("Loading and processing data... Please wait."):
        try:
            # 1. Load HCP Data (CSV) from bytes
            try:
                hcp_file_like = io.BytesIO(hcp_upload_bytes)
                hcp_df = pd.read_csv(hcp_file_like)
                # Standardize column names to lowercase for easier access
                hcp_df.columns = hcp_df.columns.str.lower()
                st.write(f"HCP data loaded: {hcp_df.shape} rows, {hcp_df.shape[1]} columns.")

                # --- Basic HCP Validation ---
                required_hcp_cols =
                if not all(col in hcp_df.columns for col in required_hcp_cols):
                    st.error(f"Error: HCP CSV must contain columns (case-insensitive): {', '.join(required_hcp_cols)}. Found: {hcp_df.columns.tolist()}")
                    return None, None
                if hcp_df[HCP_ID_COL].isnull().any():
                    st.warning(f"HCP data contains missing values in the ID column '{HCP_ID_COL}'. Rows with missing IDs will be dropped.")
                    hcp_df = hcp_df.dropna(subset=[HCP_ID_COL])
                # Check for ZIP column (needed later for potential fallback merge)
                if HCP_ZIP_COL not in hcp_df.columns:
                     st.warning(f"HCP data missing '{HCP_ZIP_COL}' column. Aggregation might be incomplete if lat/lon are also missing.")
                else:
                     if hcp_df[HCP_ZIP_COL].isnull().any():
                         st.warning(f"HCP data contains missing values in the ZIP column '{HCP_ZIP_COL}'. Rows with missing ZIPs will be dropped if used for merging.")
                         hcp_df = hcp_df.dropna(subset=[HCP_ZIP_COL])
                     # Convert ZIP to string for consistent merging
                     hcp_df[HCP_ZIP_COL] = hcp_df[HCP_ZIP_COL].astype(str).str.zfill(5)


                # Attempt to create GeoDataFrame if lat/lon exist
                if HCP_LAT_COL in hcp_df.columns and HCP_LON_COL in hcp_df.columns:
                     # Convert to numeric, coercing errors
                     hcp_df = pd.to_numeric(hcp_df, errors='coerce')
                     hcp_df = pd.to_numeric(hcp_df, errors='coerce')
                     if hcp_df.isnull().any() or hcp_df.isnull().any():
                         st.warning("HCP data contains missing or non-numeric latitude/longitude values. These rows cannot be geocoded.")
                         # Keep rows without geometry for potential attribute join later
                         valid_coords = hcp_df.notna() & hcp_df.notna()
                         geometry = [Point(xy) if valid else None for valid, xy in zip(valid_coords, zip(hcp_df, hcp_df))]
                         hcp_gdf = gpd.GeoDataFrame(hcp_df, geometry=geometry, crs="EPSG:4326") # Assume WGS84
                     else:
                         hcp_gdf = gpd.GeoDataFrame(
                             hcp_df,
                             geometry=gpd.points_from_xy(hcp_df[HCP_LON_COL], hcp_df),
                             crs="EPSG:4326" # Assume WGS84
                         )
                     # Ensure target CRS
                     try:
                         if hcp_gdf.crs!= TARGET_CRS:
                             hcp_gdf = hcp_gdf.to_crs(TARGET_CRS)
                         hcp_data_out = hcp_gdf # Assign GeoDataFrame
                     except Exception as e:
                         st.error(f"Error setting CRS for HCP data: {e}. Proceeding without geometry.")
                         hcp_data_out = hcp_df # Fallback to DataFrame
                else:
                    st.info("HCP data does not contain 'latitude' and 'longitude' columns. Proceeding without point geometries for HCPs.")
                    hcp_data_out = hcp_df # Assign DataFrame

            except pd.errors.EmptyDataError:
                st.error("The uploaded HCP CSV file is empty.")
                return None, None
            except UnicodeDecodeError:
                st.error("Could not decode the HCP CSV file. Please ensure it is UTF-8 encoded.")
                return None, None
            except Exception as e:
                st.error(f"An error occurred while loading the HCP CSV: {e}")
                return None, None

            # 2. Load ZCTA Data (Shapefile from ZIP bytes)
            try:
                temp_dir_obj = tempfile.TemporaryDirectory()
                temp_dir_path = temp_dir_obj.name
                zcta_zip_like = io.BytesIO(zcta_upload_bytes)
                with zipfile.ZipFile(zcta_zip_like, 'r') as zip_ref:
                    zip_ref.extractall(temp_dir_path)
                    st.write(f"Extracted ZIP contents to temporary directory: {temp_dir_path}") # Debug info

                # Find the shapefile within the extracted directory
                shp_file_path = None
                for root, dirs, files in os.walk(temp_dir_path):
                    for file in files:
                        if file.lower().endswith(".shp"):
                            shp_file_path = os.path.join(root, file)
                            st.write(f"Found shapefile: {shp_file_path}") # Debug info
                            break
                    if shp_file_path:
                        break

                if not shp_file_path:
                    st.error("No.shp file found in the uploaded ZIP archive.")
                    return None, None

                zcta_gdf = gpd.read_file(shp_file_path)
                st.write(f"ZCTA data loaded: {zcta_gdf.shape} rows, {zcta_gdf.shape[1]} columns.") # Debug info

                # --- Basic ZCTA Validation ---
                zcta_col_to_use = None
                # Standardize ZCTA columns to lowercase for case-insensitive check
                zcta_gdf.columns = zcta_gdf.columns.str.lower()
                zcta_col_primary_lower = ZCTA_COL_PRIMARY.lower()

                if zcta_col_primary_lower in zcta_gdf.columns:
                    zcta_col_to_use = zcta_col_primary_lower
                else:
                    # Attempt common alternatives if primary not found
                    alt_zcta_cols = # Add more common names if needed (lowercase)
                    for col in alt_zcta_cols:
                        if col in zcta_gdf.columns:
                            st.warning(f"Primary ZCTA column '{ZCTA_COL_PRIMARY}' not found. Using alternative column '{col}' as ZCTA identifier.")
                            zcta_col_to_use = col
                            break
                    if not zcta_col_to_use:
                         st.error(f"Could not find primary ZCTA column '{ZCTA_COL_PRIMARY}' or common alternatives. Found columns: {', '.join(zcta_gdf.columns)}")
                         return None, None

                # Rename the identified column to a standard 'zcta' name for consistency
                zcta_gdf = zcta_gdf.rename(columns={zcta_col_to_use: 'zcta'})

                if zcta_gdf.isnull().any():
                    st.warning(f"ZCTA data contains missing values in the identifier column '{zcta_col_to_use}'. Rows with missing identifiers will be dropped.")
                    zcta_gdf = zcta_gdf.dropna(subset=)

                # Ensure ZCTA column is string type for merging
                zcta_gdf = zcta_gdf.astype(str).str.zfill(5)

                # Check and set CRS
                if zcta_gdf.crs is None:
                    st.warning("ZCTA shapefile has no CRS defined. Assuming WGS 84 (EPSG:4326). Accurate spatial analysis requires a correct CRS.")
                    try:
                         zcta_gdf.set_crs(TARGET_CRS, inplace=True)
                    except Exception as e:
                         st.error(f"Failed to set assumed CRS: {e}")
                         return None, None
                elif zcta_gdf.crs!= TARGET_CRS:
                    st.info(f"Reprojecting ZCTA data from {zcta_gdf.crs} to {TARGET_CRS}.")
                    try:
                        zcta_gdf = zcta_gdf.to_crs(TARGET_CRS)
                    except Exception as e:
                        st.error(f"Error reprojecting ZCTA data: {e}")
                        return None, None

                # Ensure geometries are valid polygons
                zcta_gdf = zcta_gdf[zcta_gdf.geometry.notna()]
                zcta_gdf = zcta_gdf[zcta_gdf.geometry.is_valid]
                zcta_gdf = zcta_gdf[zcta_gdf.geometry.type.isin(['Polygon', 'MultiPolygon'])]
                if zcta_gdf.empty:
                    st.error("No valid Polygon or MultiPolygon geometries found in the ZCTA file after cleaning.")
                    return None, None

                # Keep only essential ZCTA columns + geometry
                # *** THIS IS THE CORRECTED LINE ***
                zcta_gdf = zcta_gdf].copy()

            except zipfile.BadZipFile:
                st.error("The uploaded ZCTA file is not a valid ZIP archive.")
                return None, None
            except Exception as e:
                st.error(f"An error occurred while loading the ZCTA data: {e}")
                return None, None

            # 3. Initial Aggregation (HCP Count per ZCTA)
            if hcp_data_out is not None and HCP_ZIP_COL in hcp_data_out.columns: # Check if hcp_data_out was loaded and has ZIP
                hcp_counts_per_zip = hcp_data_out.groupby(HCP_ZIP_COL).size().reset_index(name='hcp_count')
                hcp_counts_per_zip = hcp_counts_per_zip.rename(columns={HCP_ZIP_COL: 'zcta'})

                # Merge counts onto ZCTA data
                original_zcta_count = len(zcta_gdf)
                zcta_gdf = zcta_gdf.merge(hcp_counts_per_zip, on='zcta', how='left')
                # Fill NaNs resulting from ZCTAs with no HCPs
                zcta_gdf['hcp_count'] = zcta_gdf['hcp_count'].fillna(0).astype(int)

                if len(zcta_gdf)!= original_zcta_count:
                    st.warning("Number of ZCTAs changed during merge with HCP counts. Check for potential issues with ZCTA identifiers.")

            else:
                 st.warning("HCP data failed to load or missing ZIP column, cannot calculate HCP counts per ZCTA.")
                 # Add an empty hcp_count column if it doesn't exist
                 if 'hcp_count' not in zcta_gdf.columns:
                      zcta_gdf['hcp_count'] = 0


            st.success("Data loaded and pre-processed successfully!")
            return hcp_data_out, zcta_gdf

        finally:
            # Ensure temporary directory is cleaned up regardless of success or failure
            if temp_dir_obj:
                try:
                    temp_dir_obj.cleanup()
                    # st.write("Temporary directory cleaned up.") # Optional debug info
                except Exception as cleanup_error:
                    st.warning(f"Error cleaning up temporary directory: {cleanup_error}")

@st.cache_data(ttl=3600) # Cache results
def preprocess_and_aggregate(_hcp_data, _zcta_gdf):
    """
    Preprocesses HCP and ZCTA data: converts HCPs to points (if possible),
    performs spatial join, aggregates data, merges back to ZCTA,
    selects features, and scales them.

    Args:
        _hcp_data (DataFrame or GeoDataFrame): HCP data.
        _zcta_gdf (GeoDataFrame): ZCTA boundaries data (should have 'hcp_count').

    Returns:
        geopandas.GeoDataFrame: ZCTA GeoDataFrame with aggregated, scaled features,
                                or None if preprocessing fails.
    """
    if _hcp_data is None or _zcta_gdf is None:
        st.error("Input data missing for preprocessing.")
        return None

    zcta_processed = _zcta_gdf.copy() # Start with ZCTA data

    with st.spinner("Preprocessing data (Spatial Join, Aggregation, Scaling)..."):
        try:
            hcp_gdf = None
            # 1. Convert HCP DataFrame to GeoDataFrame if possible
            if isinstance(_hcp_data, pd.DataFrame) and HCP_LAT_COL in _hcp_data.columns and HCP_LON_COL in _hcp_data.columns:
                # Ensure lat/lon are numeric before creating points
                _hcp_data = pd.to_numeric(_hcp_data, errors='coerce')
                _hcp_data = pd.to_numeric(_hcp_data, errors='coerce')
                valid_coords = _hcp_data.notna() & _hcp_data.notna()

                if valid_coords.any(): # Only create geometry if some valid coords exist
                    geometry = [Point(xy) if valid else None for valid, xy in zip(valid_coords, zip(_hcp_data, _hcp_data))]
                    hcp_gdf = gpd.GeoDataFrame(_hcp_data, geometry=geometry, crs="EPSG:4326")
                    hcp_gdf = hcp_gdf[hcp_gdf.geometry.notna()] # Drop rows where geometry couldn't be created
                    if hcp_gdf.empty:
                         st.warning("No valid coordinates found in HCP data for spatial join.")
                         hcp_gdf = None
                    else:
                         # Ensure CRS Match
                         if hcp_gdf.crs!= zcta_processed.crs:
                              print(f"Reprojecting HCP data from {hcp_gdf.crs} to {zcta_processed.crs}")
                              try:
                                  hcp_gdf = hcp_gdf.to_crs(zcta_processed.crs)
                              except Exception as e:
                                  st.error(f"Error during HCP CRS reprojection: {e}. Cannot perform spatial join.")
                                  hcp_gdf = None # Prevent spatial join if reprojection fails
            elif isinstance(_hcp_data, gpd.GeoDataFrame):
                 hcp_gdf = _hcp_data # Already a GeoDataFrame
                 # Ensure CRS Match
                 if hcp_gdf.crs!= zcta_processed.crs:
                      print(f"Reprojecting HCP data from {hcp_gdf.crs} to {zcta_processed.crs}")
                      try:
                          hcp_gdf = hcp_gdf.to_crs(zcta_processed.crs)
                      except Exception as e:
                          st.error(f"Error during HCP CRS reprojection: {e}. Cannot perform spatial join.")
                          hcp_gdf = None

            # 2. Spatial Join (if HCP geometry exists)
            hcp_with_zcta = None
            if hcp_gdf is not None and not hcp_gdf.empty:
                print("Performing spatial join...")
                # Ensure geometry columns have unique names before join if needed
                if 'geometry' in zcta_processed.columns and zcta_processed.geometry.name == hcp_gdf.geometry.name:
                     zcta_processed = zcta_processed.rename_geometry('zcta_geom')

                hcp_with_zcta = gpd.sjoin(hcp_gdf, zcta_processed], how='inner', predicate='within') # Use op='within' for newer geopandas
                print(f"HCPs joined with ZCTAs: {len(hcp_with_zcta)}")
                if len(hcp_with_zcta) == 0:
                    st.warning("No HCPs were successfully joined to ZCTAs via spatial join. Aggregation might be incomplete.")
            else:
                 st.warning("Skipping spatial join as HCP geometry is not available or invalid.")


            # 3. Aggregation by ZCTA
            print("Aggregating HCP data by ZCTA...")
            # Use spatially joined data if available, otherwise fall back to original HCP data with ZIP
            source_df_for_agg = hcp_with_zcta if hcp_with_zcta is not None and not hcp_with_zcta.empty else _hcp_data
            grouping_col = 'zcta' if 'zcta' in source_df_for_agg.columns else HCP_ZIP_COL

            if grouping_col not in source_df_for_agg.columns:
                 st.error(f"Cannot aggregate: Missing grouping column ('zcta' or '{HCP_ZIP_COL}') in source data.")
                 return None

            # Define aggregations - ensure columns exist
            agg_dict = {}
            if HCP_ID_COL in source_df_for_agg.columns:
                 agg_dict['hcp_count_agg'] = (HCP_ID_COL, 'nunique')
            if HCP_TRX_COL in source_df_for_agg.columns:
                 # Ensure TRX col is numeric before aggregation
                 source_df_for_agg = pd.to_numeric(source_df_for_agg, errors='coerce')
                 agg_dict['total_trx_volume_agg'] = (HCP_TRX_COL, 'sum')

            if not agg_dict:
                 st.warning("No columns found for aggregation (checked for hcp_id, trx_volume).")
                 agg_data = pd.DataFrame({'zcta': zcta_processed.unique()}) # Create empty agg df
            else:
                 agg_data = source_df_for_agg.groupby(grouping_col).agg(**agg_dict).reset_index()
                 # Rename grouping col back to zcta if needed
                 if grouping_col == HCP_ZIP_COL:
                      agg_data = agg_data.rename(columns={HCP_ZIP_COL: 'zcta'})

            print(f"Aggregated data for {len(agg_data)} ZCTAs.")

            # 4. Attribute Join back to ZCTA boundaries
            print("Merging aggregated data back to ZCTA boundaries...")
            # Drop old aggregation columns if they exist before merge to avoid conflicts
            cols_to_drop = ['hcp_count', 'total_trx_volume'] # Drop original count if exists
            zcta_processed = zcta_processed.drop(columns=cols_to_drop, errors='ignore')
            # Merge new aggregations
            zcta_processed = zcta_processed.merge(agg_data, on='zcta', how='left')

            # Fill NaNs and rename aggregated columns
            if 'hcp_count_agg' in zcta_processed.columns:
                 zcta_processed['hcp_count'] = zcta_processed['hcp_count_agg'].fillna(0).astype(int)
                 zcta_processed = zcta_processed.drop(columns=['hcp_count_agg'])
            elif 'hcp_count' not in zcta_processed.columns: # Ensure column exists even if aggregation failed
                 zcta_processed['hcp_count'] = 0

            if 'total_trx_volume_agg' in zcta_processed.columns:
                 zcta_processed['total_trx_volume'] = zcta_processed['total_trx_volume_agg'].fillna(0)
                 zcta_processed = zcta_processed.drop(columns=['total_trx_volume_agg'])
            elif 'total_trx_volume' not in zcta_processed.columns:
                 zcta_processed['total_trx_volume'] = 0.0


            # 5. Feature Selection and Scaling (for K-Means and SKATER)
            features_to_scale = ['hcp_count', 'total_trx_volume']
            if not all(f in zcta_processed.columns for f in features_to_scale):
                 st.warning(f"Could not find all features ({', '.join(features_to_scale)}) for scaling after aggregation.")
                 # Only scale available features
                 features_to_scale = [f for f in features_to_scale if f in zcta_processed.columns]
                 if not features_to_scale:
                      st.error("No numeric features available for scaling/clustering.")
                      return None

            print(f"Scaling features: {', '.join(features_to_scale)}")
            scaler = StandardScaler()
            imputer = SimpleImputer(strategy='mean') # Use mean imputation before scaling
            zcta_processed[features_to_scale] = imputer.fit_transform(zcta_processed[features_to_scale])
            # Apply scaling
            zcta_processed[[f + '_scaled' for f in features_to_scale]] = scaler.fit_transform(zcta_processed[features_to_scale])

            print("Preprocessing complete.")
            return zcta_processed

        except Exception as e:
            st.error(f"Error during preprocessing: {e}")
            print(f"Error during preprocessing: {e}")
            import traceback
            traceback.print_exc()
            return None

@st.cache_data(ttl=3600) # Cache results
def segment_zctas(_gdf, features_scaled, n_clusters=5):
    """Performs K-Means clustering on the ZCTA GeoDataFrame."""
    if _gdf is None or _gdf.empty:
        st.error("Input GeoDataFrame for segmentation is invalid.")
        return None
    if not features_scaled or not all(f in _gdf.columns for f in features_scaled):
        st.error(f"Error: Scaled features ({', '.join(features_scaled)}) not found in GeoDataFrame.")
        return None
    if n_clusters <= 1 or n_clusters >= len(_gdf): # K must be > 1 and < N samples
        st.error(f"Error: Invalid number of clusters ({n_clusters}). Must be > 1 and < number of ZCTAs.")
        return None

    gdf_segmented = _gdf.copy() # Work on a copy

    try:
        print(f"Performing K-Means segmentation into {n_clusters} clusters...")
        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10) # Use n_init=10 or 'auto'
        gdf_segmented['segment'] = kmeans.fit_predict(gdf_segmented[features_scaled])
        gdf_segmented['segment'] = gdf_segmented['segment'] + 1 # 1-based labels
        print("K-Means segmentation complete.")
        # Don't show success message here, do it in the main app flow
        return gdf_segmented
    except Exception as e:
        st.error(f"Error during K-Means segmentation: {e}")
        print(f"Error during K-Means segmentation: {e}")
        return None

@st.cache_data(ttl=3600) # Cache results
def calculate_spatial_weights(_zcta_gdf):
    """Calculates spatial weights matrix using Queen contiguity."""
    if _zcta_gdf is None or 'geometry' not in _zcta_gdf.columns or _zcta_gdf.empty:
        st.warning("Cannot calculate spatial weights. ZCTA GeoDataFrame is missing or invalid.")
        return None, None

    gdf_weights = _zcta_gdf.copy() # Work on a copy

    with st.spinner("Calculating spatial weights (Queen Contiguity)..."):
        try:
            # Ensure geometry column is active and clean invalid geometries
            if gdf_weights.geometry.name!= 'geometry':
                 gdf_weights = gdf_weights.set_geometry('geometry')
            gdf_weights = gdf_weights[gdf_weights.geometry.notna() & gdf_weights.geometry.is_valid & ~gdf_weights.geometry.is_empty]
            if gdf_weights.empty:
                 st.error("No valid geometries remaining after cleaning for weights calculation.")
                 return None, None

            # Ensure unique index for weights calculation
            if not gdf_weights.index.is_unique:
                gdf_weights = gdf_weights.reset_index(drop=True)

            w = libpysal.weights.Queen.from_dataframe(gdf_weights, ids=gdf_weights.index.tolist(), silence_warnings=True)

            if w.n_components > 1:
                st.warning(f"Spatial weights calculation identified {w.n_components} disconnected components (islands). SKATER might behave unexpectedly.")

            st.success("Spatial weights calculated successfully.")
            return w, gdf_weights # Return potentially cleaned GDF as well

        except Exception as e:
            st.error(f"An error occurred during spatial weights calculation: {e}")
            print(f"Error creating spatial weights: {e}")
            return None, None

@st.cache_data(ttl=3600) # Cache results
def optimize_territories(_gdf, w, features_scaled, n_territories=10, min_territory_size=1):
    """Optimizes territories using the SKATER algorithm."""
    if _gdf is None or _gdf.empty or 'geometry' not in _gdf.columns:
        st.error("Input GeoDataFrame for territory optimization is invalid.")
        return None
    if w is None:
        st.error("Spatial weights object is missing for territory optimization.")
        return None
    if not features_scaled or not all(f in _gdf.columns for f in features_scaled):
        st.error(f"Error: Scaled features ({', '.join(features_scaled)}) not found.")
        return None
    if n_territories <= 1 or n_territories >= len(_gdf): # Must be > 1 and < N
        st.error(f"Error: Invalid number of territories ({n_territories}). Must be > 1 and < number of ZCTAs.")
        return None
    if min_territory_size <= 0:
        st.error(f"Error: Minimum territory size ({min_territory_size}) must be > 0.")
        return None

    territory_gdf = _gdf.copy() # Work on a copy

    try:
        # Ensure index alignment between GDF and W if GDF was cleaned during weights calc
        if territory_gdf.index.tolist()!= list(w.neighbors.keys()): # Convert keys to list for comparison
             st.warning("Index mismatch between GeoDataFrame and Weights object. Realigning GDF index.")
             try:
                 territory_gdf = territory_gdf.loc[list(w.neighbors.keys())].copy() # Convert keys to list
             except KeyError:
                  st.error("Failed to realign GeoDataFrame index with weights. Check for inconsistencies.")
                  return None
             if territory_gdf.empty:
                  st.error("GeoDataFrame became empty after aligning with weights index.")
                  return None

        #