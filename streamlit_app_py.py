# -*- coding: utf-8 -*-
"""streamlit_app.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10NNokAqvf8SiRfevbf707EO29ZrHMoBD
"""

# -*- coding: utf-8 -*-
"""
streamlit_app.py

Streamlit application for Healthcare Provider (HCP) Segmentation and
Sales Territory Optimization.

This script implements an interactive web application using Streamlit
for pharmaceutical sales operations. It allows users to:
1. Upload HCP data (CSV) and ZCTA boundary data (Shapefile in ZIP).
2. Perform K-Means clustering on ZCTA-aggregated data for segmentation based on
   user-selected features and cluster count.
3. Perform territory optimization using the SKATER algorithm on ZCTAs,
   aiming for a specified number of contiguous territories, balancing
   based on a selected ZCTA attribute, and respecting a minimum size constraint.
4. Visualize the optimized territories and optionally the HCP segments
   on an interactive map using Folium.
5. Display summary statistics and data previews for both segmentation
   and territory optimization results in separate tabs.
6. Allow users to download the optimized territory data (excluding geometry) as CSV.
"""

import streamlit as st
import pandas as pd
import geopandas as gpd
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer # Used for handling NaNs before scaling
import libpysal.weights
from spopt.region import Skater
import folium
from streamlit_folium import st_folium
import zipfile
import tempfile
import os
import io
import warnings
import matplotlib.pyplot as plt
import numpy as np
import matplotlib.cm as cm # Added for colormap generation
import matplotlib.colors as mcolors # Added for colormap generation
from shapely.geometry import Point
import shutil # For cleaning up temp dir

# --- Configuration ---

# Suppress specific warnings for cleaner output
warnings.filterwarnings('ignore', category=FutureWarning)
warnings.filterwarnings('ignore', category=UserWarning)
warnings.filterwarnings('ignore', message="The weights matrix is not fully connected") # Common with real-world data

# Target Coordinate Reference System (CRS) for all spatial data.
# Using WGS 84 (latitude/longitude) for Folium compatibility.
TARGET_CRS = 'EPSG:4326'

# Default column names (adjust if your input files use different names)
HCP_ID_COL = 'hcp_id' # Unique identifier for HCPs
HCP_LAT_COL = 'latitude'
HCP_LON_COL = 'longitude'
HCP_TRX_COL = 'trx_volume' # Example feature column
# Column containing ZCTA codes in the shapefile (common TIGER/Line name)
# Will try common alternatives if this exact name isn't found.
ZCTA_COL_PRIMARY = 'ZCTA5CE20'

# Default map settings
DEFAULT_MAP_LOCATION = [39.8283, -98.5795] # Center of the US
DEFAULT_MAP_ZOOM = 4

# --- Helper Functions ---

def generate_colormap(num_colors):
    """Generates a discrete colormap for visualization."""
    # Using a qualitative map like 'tab20' for distinct categories
    if num_colors <= 10:
        cmap = cm.get_cmap('tab10', num_colors)
    elif num_colors <= 20:
        cmap = cm.get_cmap('tab20', num_colors)
    else:
        # Fallback for more colors using viridis, might not be visually distinct
        cmap = cm.get_cmap('viridis', num_colors)
    return [mcolors.to_hex(cmap(i)) for i in range(num_colors)]

@st.cache_data(ttl=3600) # Cache data loading for 1 hour
def load_data(hcp_upload_bytes, zcta_upload_bytes):
    """
    Loads HCP data from CSV bytes and ZCTA boundaries from ZIP bytes.
    Performs validation, cleaning, CRS transformation, and initial aggregation.

    Args:
        hcp_upload_bytes (bytes): Bytes content of the HCP CSV file.
        zcta_upload_bytes (bytes): Bytes content of the ZCTA ZIP file.

    Returns:
        tuple: (GeoDataFrame/DataFrame, GeoDataFrame) containing HCP data and ZCTA data,
               or (None, None) if loading fails. Returns ZCTA gdf with an
               'hcp_count' column added. HCP data is returned as GeoDataFrame
               if lat/lon are present, otherwise as DataFrame.
    """
    if not hcp_upload_bytes or not zcta_upload_bytes:
        return None, None

    hcp_data_out = None
    zcta_gdf = None
    temp_dir_obj = None # Use TemporaryDirectory for automatic cleanup

    with st.spinner("Loading and processing data... Please wait."):
        try:
            # 1. Load HCP Data (CSV) from bytes
            try:
                hcp_file_like = io.BytesIO(hcp_upload_bytes)
                hcp_df = pd.read_csv(hcp_file_like)
                st.write(f"HCP data loaded: {hcp_df.shape} rows, {hcp_df.shape[1]} columns.")

                # --- Basic HCP Validation ---
                required_hcp_cols =
                if not all(col in hcp_df.columns for col in required_hcp_cols):
                    st.error(f"Error: HCP CSV must contain columns: {', '.join(required_hcp_cols)}")
                    return None, None
                if hcp_df[HCP_ID_COL].isnull().any():
                    st.warning(f"HCP data contains missing values in the ID column '{HCP_ID_COL}'. Rows with missing IDs will be dropped.")
                    hcp_df = hcp_df.dropna(subset=[HCP_ID_COL])
                # Check for ZIP column (needed later for potential fallback merge)
                if HCP_ZIP_COL not in hcp_df.columns:
                     st.warning(f"HCP data missing '{HCP_ZIP_COL}' column. Aggregation might be incomplete if lat/lon are also missing.")
                else:
                     if hcp_df[HCP_ZIP_COL].isnull().any():
                         st.warning(f"HCP data contains missing values in the ZIP column '{HCP_ZIP_COL}'. Rows with missing ZIPs will be dropped if used for merging.")
                         hcp_df = hcp_df.dropna(subset=[HCP_ZIP_COL])
                     # Convert ZIP to string for consistent merging
                     hcp_df[HCP_ZIP_COL] = hcp_df[HCP_ZIP_COL].astype(str).str.zfill(5)


                # Attempt to create GeoDataFrame if lat/lon exist
                if HCP_LAT_COL in hcp_df.columns and HCP_LON_COL in hcp_df.columns:
                     if hcp_df.isnull().any() or hcp_df.isnull().any():
                         st.warning("HCP data contains missing latitude/longitude values. These rows cannot be geocoded.")
                         # Convert valid points, keep others as DataFrame rows
                         hcp_df = pd.to_numeric(hcp_df, errors='coerce')
                         hcp_df = pd.to_numeric(hcp_df, errors='coerce')
                         valid_coords = hcp_df.notna() & hcp_df.notna()
                         geometry = [Point(xy) if valid else None for valid, xy in zip(valid_coords, zip(hcp_df, hcp_df))]
                         hcp_gdf = gpd.GeoDataFrame(hcp_df, geometry=geometry, crs="EPSG:4326") # Assume WGS84
                     else:
                         hcp_gdf = gpd.GeoDataFrame(
                             hcp_df,
                             geometry=gpd.points_from_xy(hcp_df[HCP_LON_COL], hcp_df),
                             crs="EPSG:4326" # Assume WGS84
                         )
                     # Ensure target CRS
                     try:
                         if hcp_gdf.crs!= TARGET_CRS:
                             hcp_gdf = hcp_gdf.to_crs(TARGET_CRS)
                         hcp_data_out = hcp_gdf # Assign GeoDataFrame
                     except Exception as e:
                         st.error(f"Error setting CRS for HCP data: {e}. Proceeding without geometry.")
                         hcp_data_out = hcp_df # Fallback to DataFrame
                else:
                    st.info("HCP data does not contain 'latitude' and 'longitude' columns. Proceeding without point geometries for HCPs.")
                    hcp_data_out = hcp_df # Assign DataFrame

            except pd.errors.EmptyDataError:
                st.error("The uploaded HCP CSV file is empty.")
                return None, None
            except UnicodeDecodeError:
                st.error("Could not decode the HCP CSV file. Please ensure it is UTF-8 encoded.")
                return None, None
            except Exception as e:
                st.error(f"An error occurred while loading the HCP CSV: {e}")
                return None, None

            # 2. Load ZCTA Data (Shapefile from ZIP bytes)
            try:
                temp_dir_obj = tempfile.TemporaryDirectory()
                temp_dir_path = temp_dir_obj.name
                zcta_zip_like = io.BytesIO(zcta_upload_bytes)
                with zipfile.ZipFile(zcta_zip_like, 'r') as zip_ref:
                    zip_ref.extractall(temp_dir_path)
                    st.write(f"Extracted ZIP contents to temporary directory: {temp_dir_path}") # Debug info

                # Find the shapefile within the extracted directory
                shp_file_path = None
                for root, dirs, files in os.walk(temp_dir_path):
                    for file in files:
                        if file.lower().endswith(".shp"):
                            shp_file_path = os.path.join(root, file)
                            st.write(f"Found shapefile: {shp_file_path}") # Debug info
                            break
                    if shp_file_path:
                        break

                if not shp_file_path:
                    st.error("No.shp file found in the uploaded ZIP archive.")
                    return None, None

                zcta_gdf = gpd.read_file(shp_file_path)
                st.write(f"ZCTA data loaded: {zcta_gdf.shape} rows, {zcta_gdf.shape[1]} columns.") # Debug info

                # --- Basic ZCTA Validation ---
                zcta_col_to_use = None
                if ZCTA_COL_PRIMARY in zcta_gdf.columns:
                    zcta_col_to_use = ZCTA_COL_PRIMARY
                else:
                    # Attempt common alternatives if primary not found
                    alt_zcta_cols = # Add more common names if needed
                    for col in alt_zcta_cols:
                        if col in zcta_gdf.columns:
                            st.warning(f"Primary ZCTA column '{ZCTA_COL_PRIMARY}' not found. Using alternative column '{col}' as ZCTA identifier.")
                            zcta_col_to_use = col
                            break
                    if not zcta_col_to_use:
                         st.error(f"Could not find primary ZCTA column '{ZCTA_COL_PRIMARY}' or common alternatives. Found columns: {', '.join(zcta_gdf.columns)}")
                         return None, None

                # Rename the identified column to a standard 'ZCTA' name for consistency
                zcta_gdf = zcta_gdf.rename(columns={zcta_col_to_use: 'ZCTA'})

                if zcta_gdf.isnull().any():
                    st.warning(f"ZCTA data contains missing values in the identifier column '{zcta_col_to_use}'. Rows with missing identifiers will be dropped.")
                    zcta_gdf = zcta_gdf.dropna(subset=)

                # Ensure ZCTA column is string type for merging
                zcta_gdf = zcta_gdf.astype(str).str.zfill(5)

                # Check and set CRS
                if zcta_gdf.crs is None:
                    st.warning("ZCTA shapefile has no CRS defined. Assuming WGS 84 (EPSG:4326). Accurate spatial analysis requires a correct CRS.")
                    try:
                         zcta_gdf.set_crs(TARGET_CRS, inplace=True)
                    except Exception as e:
                         st.error(f"Failed to set assumed CRS: {e}")
                         return None, None
                elif zcta_gdf.crs!= TARGET_CRS:
                    st.info(f"Reprojecting ZCTA data from {zcta_gdf.crs} to {TARGET_CRS}.")
                    try:
                        zcta_gdf = zcta_gdf.to_crs(TARGET_CRS)
                    except Exception as e:
                        st.error(f"Error reprojecting ZCTA data: {e}")
                        return None, None

                # Ensure geometries are valid polygons
                zcta_gdf = zcta_gdf[zcta_gdf.geometry.notna()]
                zcta_gdf = zcta_gdf[zcta_gdf.geometry.is_valid]
                zcta_gdf = zcta_gdf[zcta_gdf.geometry.type.isin(['Polygon', 'MultiPolygon'])]
                if zcta_gdf.empty:
                    st.error("No valid Polygon or MultiPolygon geometries found in the ZCTA file after cleaning.")
                    return None, None

                # Keep only essential ZCTA columns + geometry
                zcta_gdf = zcta_gdf].copy()

            except zipfile.BadZipFile:
                st.error("The uploaded ZCTA file is not a valid ZIP archive.")
                return None, None
            except Exception as e:
                st.error(f"An error occurred while loading the ZCTA data: {e}")
                return None, None

            # 3. Initial Aggregation (HCP Count per ZCTA)
            if hcp_data_out is not None and HCP_ZIP_COL in hcp_data_out.columns: # Check if hcp_data_out was loaded and has ZIP
                hcp_counts_per_zip = hcp_data_out.groupby(HCP_ZIP_COL).size().reset_index(name='hcp_count')
                hcp_counts_per_zip = hcp_counts_per_zip.rename(columns={HCP_ZIP_COL: 'ZCTA'})

                # Merge counts onto ZCTA data
                original_zcta_count = len(zcta_gdf)
                zcta_gdf = zcta_gdf.merge(hcp_counts_per_zip, on='ZCTA', how='left')
                # Fill NaNs resulting from ZCTAs with no HCPs
                zcta_gdf['hcp_count'] = zcta_gdf['hcp_count'].fillna(0).astype(int)

                if len(zcta_gdf)!= original_zcta_count:
                    st.warning("Number of ZCTAs changed during merge with HCP counts. Check for potential issues with ZCTA identifiers.")

            else:
                 st.warning("HCP data failed to load or missing ZIP column, cannot calculate HCP counts per ZCTA.")
                 # Add an empty hcp_count column if it doesn't exist
                 if 'hcp_count' not in zcta_gdf.columns:
                      zcta_gdf['hcp_count'] = 0


            st.success("Data loaded and pre-processed successfully!")
            return hcp_data_out, zcta_gdf

        finally:
            # Ensure temporary directory is cleaned up regardless of success or failure
            if temp_dir_obj:
                try:
                    temp_dir_obj.cleanup()
                    # st.write("Temporary directory cleaned up.") # Optional debug info
                except Exception as cleanup_error:
                    st.warning(f"Error cleaning up temporary directory: {cleanup_error}")

@st.cache_data(ttl=3600) # Cache results
def preprocess_and_aggregate(_hcp_data, _zcta_gdf):
    """
    Preprocesses HCP and ZCTA data: converts HCPs to points (if possible),
    performs spatial join, aggregates data, merges back to ZCTA,
    selects features, and scales them.

    Args:
        _hcp_data (DataFrame or GeoDataFrame): HCP data.
        _zcta_gdf (GeoDataFrame): ZCTA boundaries data (should have 'hcp_count').

    Returns:
        geopandas.GeoDataFrame: ZCTA GeoDataFrame with aggregated, scaled features,
                                or None if preprocessing fails.
    """
    if _hcp_data is None or _zcta_gdf is None:
        st.error("Input data missing for preprocessing.")
        return None

    zcta_processed = _zcta_gdf.copy() # Start with ZCTA data

    with st.spinner("Preprocessing data (Spatial Join, Aggregation, Scaling)..."):
        try:
            hcp_gdf = None
            # 1. Convert HCP DataFrame to GeoDataFrame if possible
            if isinstance(_hcp_data, pd.DataFrame) and HCP_LAT_COL in _hcp_data.columns and HCP_LON_COL in _hcp_data.columns:
                valid_coords = _hcp_data.notna() & _hcp_data.notna()
                if valid_coords.any(): # Only create geometry if some valid coords exist
                    geometry = [Point(xy) if valid else None for valid, xy in zip(valid_coords, zip(_hcp_data, _hcp_data))]
                    hcp_gdf = gpd.GeoDataFrame(_hcp_data, geometry=geometry, crs="EPSG:4326")
                    hcp_gdf = hcp_gdf[hcp_gdf.geometry.notna()] # Drop rows where geometry couldn't be created
                    if hcp_gdf.empty:
                         st.warning("No valid coordinates found in HCP data for spatial join.")
                         hcp_gdf = None
                    else:
                         # Ensure CRS Match
                         if hcp_gdf.crs!= zcta_processed.crs:
                              print(f"Reprojecting HCP data from {hcp_gdf.crs} to {zcta_processed.crs}")
                              try:
                                  hcp_gdf = hcp_gdf.to_crs(zcta_processed.crs)
                              except Exception as e:
                                  st.error(f"Error during HCP CRS reprojection: {e}. Cannot perform spatial join.")
                                  hcp_gdf = None # Prevent spatial join if reprojection fails
            elif isinstance(_hcp_data, gpd.GeoDataFrame):
                 hcp_gdf = _hcp_data # Already a GeoDataFrame
                 # Ensure CRS Match
                 if hcp_gdf.crs!= zcta_processed.crs:
                      print(f"Reprojecting HCP data from {hcp_gdf.crs} to {zcta_processed.crs}")
                      try:
                          hcp_gdf = hcp_gdf.to_crs(zcta_processed.crs)
                      except Exception as e:
                          st.error(f"Error during HCP CRS reprojection: {e}. Cannot perform spatial join.")
                          hcp_gdf = None

            # 2. Spatial Join (if HCP geometry exists)
            hcp_with_zcta = None
            if hcp_gdf is not None and not hcp_gdf.empty:
                print("Performing spatial join...")
                hcp_with_zcta = gpd.sjoin(hcp_gdf, zcta_processed], how='inner', predicate='within')
                print(f"HCPs joined with ZCTAs: {len(hcp_with_zcta)}")
                if len(hcp_with_zcta) == 0:
                    st.warning("No HCPs were successfully joined to ZCTAs via spatial join. Aggregation might be incomplete.")
            else:
                 st.warning("Skipping spatial join as HCP geometry is not available or invalid.")


            # 3. Aggregation by ZCTA
            print("Aggregating HCP data by ZCTA...")
            # Use spatially joined data if available, otherwise fall back to original HCP data with ZIP
            source_df_for_agg = hcp_with_zcta if hcp_with_zcta is not None and not hcp_with_zcta.empty else _hcp_data
            grouping_col = 'ZCTA' if 'ZCTA' in source_df_for_agg.columns else HCP_ZIP_COL

            if grouping_col not in source_df_for_agg.columns:
                 st.error(f"Cannot aggregate: Missing grouping column ('ZCTA' or '{HCP_ZIP_COL}') in source data.")
                 return None

            # Define aggregations - ensure columns exist
            agg_dict = {}
            if HCP_ID_COL in source_df_for_agg.columns:
                 agg_dict['hcp_count_agg'] = (HCP_ID_COL, 'nunique')
            if HCP_TRX_COL in source_df_for_agg.columns:
                 agg_dict['total_trx_volume_agg'] = (HCP_TRX_COL, 'sum')

            if not agg_dict:
                 st.warning("No columns found for aggregation (checked for hcp_id, trx_volume).")
                 agg_data = pd.DataFrame({'ZCTA': zcta_processed.unique()}) # Create empty agg df
            else:
                 agg_data = source_df_for_agg.groupby(grouping_col).agg(**agg_dict).reset_index()
                 # Rename grouping col back to ZCTA if needed
                 if grouping_col == HCP_ZIP_COL:
                      agg_data = agg_data.rename(columns={HCP_ZIP_COL: 'ZCTA'})

            print(f"Aggregated data for {len(agg_data)} ZCTAs.")

            # 4. Attribute Join back to ZCTA boundaries
            print("Merging aggregated data back to ZCTA boundaries...")
            # Drop old aggregation columns if they exist before merge to avoid conflicts
            cols_to_drop = ['hcp_count', 'total_trx_volume'] # Drop original count if exists
            zcta_processed = zcta_processed.drop(columns=cols_to_drop, errors='ignore')
            # Merge new aggregations
            zcta_processed = zcta_processed.merge(agg_data, on='ZCTA', how='left')

            # Fill NaNs and rename aggregated columns
            if 'hcp_count_agg' in zcta_processed.columns:
                 zcta_processed['hcp_count'] = zcta_processed['hcp_count_agg'].fillna(0).astype(int)
                 zcta_processed = zcta_processed.drop(columns=['hcp_count_agg'])
            elif 'hcp_count' not in zcta_processed.columns: # Ensure column exists even if aggregation failed
                 zcta_processed['hcp_count'] = 0

            if 'total_trx_volume_agg' in zcta_processed.columns:
                 zcta_processed['total_trx_volume'] = zcta_processed['total_trx_volume_agg'].fillna(0)
                 zcta_processed = zcta_processed.drop(columns=['total_trx_volume_agg'])
            elif 'total_trx_volume' not in zcta_processed.columns:
                 zcta_processed['total_trx_volume'] = 0.0


            # 5. Feature Selection and Scaling (for K-Means and SKATER)
            features_to_scale = ['hcp_count', 'total_trx_volume']
            if not all(f in zcta_processed.columns for f in features_to_scale):
                 st.warning(f"Could not find all features ({', '.join(features_to_scale)}) for scaling after aggregation.")
                 # Only scale available features
                 features_to_scale = [f for f in features_to_scale if f in zcta_processed.columns]
                 if not features_to_scale:
                      st.error("No numeric features available for scaling/clustering.")
                      return None

            print(f"Scaling features: {', '.join(features_to_scale)}")
            scaler = StandardScaler()
            imputer = SimpleImputer(strategy='mean') # Use mean imputation before scaling
            zcta_processed[features_to_scale] = imputer.fit_transform(zcta_processed[features_to_scale])
            zcta_processed[[f + '_scaled' for f in features_to_scale]] = scaler.fit_transform(zcta_processed[features_to_scale])

            print("Preprocessing complete.")
            return zcta_processed

        except Exception as e:
            st.error(f"Error during preprocessing: {e}")
            print(f"Error during preprocessing: {e}")
            import traceback
            traceback.print_exc()
            return None

@st.cache_data(ttl=3600) # Cache results
def segment_zctas(_gdf, features_scaled, n_clusters=5):
    """Performs K-Means clustering on the ZCTA GeoDataFrame."""
    if _gdf is None or _gdf.empty:
        st.error("Input GeoDataFrame for segmentation is invalid.")
        return None
    if not features_scaled or not all(f in _gdf.columns for f in features_scaled):
        st.error(f"Error: Scaled features ({', '.join(features_scaled)}) not found in GeoDataFrame.")
        return None
    if n_clusters <= 1 or n_clusters >= len(_gdf): # K must be > 1 and < N samples
        st.error(f"Error: Invalid number of clusters ({n_clusters}). Must be > 1 and < number of ZCTAs.")
        return None

    gdf_segmented = _gdf.copy() # Work on a copy

    try:
        print(f"Performing K-Means segmentation into {n_clusters} clusters...")
        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
        gdf_segmented['segment'] = kmeans.fit_predict(gdf_segmented[features_scaled])
        gdf_segmented['segment'] = gdf_segmented['segment'] + 1 # 1-based labels
        print("K-Means segmentation complete.")
        st.success(f"K-Means segmentation completed. {n_clusters} segments identified.")
        return gdf_segmented
    except Exception as e:
        st.error(f"Error during K-Means segmentation: {e}")
        print(f"Error during K-Means segmentation: {e}")
        return None

@st.cache_data(ttl=3600) # Cache results
def calculate_spatial_weights(_zcta_gdf):
    """Calculates spatial weights matrix using Queen contiguity."""
    if _zcta_gdf is None or 'geometry' not in _zcta_gdf.columns or _zcta_gdf.empty:
        st.warning("Cannot calculate spatial weights. ZCTA GeoDataFrame is missing or invalid.")
        return None

    gdf_weights = _zcta_gdf.copy() # Work on a copy

    with st.spinner("Calculating spatial weights (Queen Contiguity)..."):
        try:
            # Ensure geometry column is active and clean invalid geometries
            if gdf_weights.geometry.name!= 'geometry':
                 gdf_weights = gdf_weights.set_geometry('geometry')
            gdf_weights = gdf_weights[gdf_weights.geometry.notna() & gdf_weights.geometry.is_valid & ~gdf_weights.geometry.is_empty]
            if gdf_weights.empty:
                 st.error("No valid geometries remaining after cleaning for weights calculation.")
                 return None

            # Ensure unique index for weights calculation
            if not gdf_weights.index.is_unique:
                gdf_weights = gdf_weights.reset_index(drop=True)

            w = libpysal.weights.Queen.from_dataframe(gdf_weights, ids=gdf_weights.index.tolist(), silence_warnings=True)

            if w.n_components > 1:
                st.warning(f"Spatial weights calculation identified {w.n_components} disconnected components (islands). SKATER might behave unexpectedly.")

            st.success("Spatial weights calculated successfully.")
            return w, gdf_weights # Return potentially cleaned GDF as well

        except Exception as e:
            st.error(f"An error occurred during spatial weights calculation: {e}")
            print(f"Error creating spatial weights: {e}")
            return None, None

@st.cache_data(ttl=3600) # Cache results
def optimize_territories(_gdf, w, features_scaled, n_territories=10, min_territory_size=1):
    """Optimizes territories using the SKATER algorithm."""
    if _gdf is None or _gdf.empty or 'geometry' not in _gdf.columns:
        st.error("Input GeoDataFrame for territory optimization is invalid.")
        return None
    if w is None:
        st.error("Spatial weights object is missing for territory optimization.")
        return None
    if not features_scaled or not all(f in _gdf.columns for f in features_scaled):
        st.error(f"Error: Scaled features ({', '.join(features_scaled)}) not found.")
        return None
    if n_territories <= 1 or n_territories >= len(_gdf): # Must be > 1 and < N
        st.error(f"Error: Invalid number of territories ({n_territories}). Must be > 1 and < number of ZCTAs.")
        return None
    if min_territory_size <= 0:
        st.error(f"Error: Minimum territory size ({min_territory_size}) must be > 0.")
        return None

    territory_gdf = _gdf.copy() # Work on a copy

    try:
        # Ensure index alignment between GDF and W if GDF was cleaned during weights calc
        if territory_gdf.index.tolist()!= w.neighbors.keys():
             st.warning("Index mismatch between GeoDataFrame and Weights object. Realigning GDF index.")
             territory_gdf = territory_gdf.loc[list(w.neighbors.keys())].copy()
             if territory_gdf.empty:
                  st.error("GeoDataFrame became empty after aligning with weights index.")
                  return None

        # Re-check territory count against potentially reduced GDF size
        if n_territories >= len(territory_gdf):
             st.error(f"Number of territories ({n_territories}) must be less than the number of valid, connected ZCTAs ({len(territory_gdf)}).")
             return None

        print(f"Running SKATER for {n_territories} territories...")
        model = Skater(
            territory_gdf,
            w,
            attrs_name=features_scaled, # Use scaled features for homogeneity
            n_clusters=n_territories,
            floor=min_territory_size,
            islands='ignore'
        )
        model.solve()

        # Add territory labels
        territory_gdf['territory'] = model.labels_ + 1 # 1-based labels
        print("SKATER territory optimization complete.")
        num_created = len(territory_gdf['territory'].unique())
        st.success(f"SKATER optimization completed. {num_created} territories created.")

        # Check floor constraint
        territory_sizes = territory_gdf.groupby('territory').size()
        if not territory_sizes.empty and territory_sizes.min() < min_territory_size:
             st.warning(f"Minimum territory size constraint ({min_territory_size}) may not have been fully met. Smallest territory size: {territory_sizes.min()}")

        return territory_gdf

    except ImportError:
        st.error("Error: PySAL `spopt` library not found. Please install it.")
        return None
    except Exception as e:
        st.error(f"Error during SKATER optimization: {e}")
        print(f"Error during SKATER optimization: {e}")
        import traceback
        traceback.print_exc()
        return None

@st.cache_data # Cache the map object generation logic
def create_map(_territory_gdf, balance_variable=None):
    """Creates a Folium map visualizing territories."""
    if _territory_gdf is None or _territory_gdf.empty or 'geometry' not in _territory_gdf.columns or 'territory' not in _territory_gdf.columns:
        st.warning("Cannot create map. Invalid GeoDataFrame.")
        return None

    with st.spinner("Generating map visualization..."):
        try:
            map_gdf = _territory_gdf.to_crs("EPSG:4326") # Ensure WGS84 for Folium

            bounds = map_gdf.total_bounds
            center_lat = (bounds[1] + bounds[2]) / 2
            center_lon = (bounds + bounds[3]) / 2
            map_location = [center_lat, center_lon]

            m = folium.Map(location=map_location, zoom_start=DEFAULT_MAP_ZOOM, tiles='CartoDB positron')

            tooltip_fields = # Basic fields
            tooltip_aliases =
            if balance_variable and balance_variable in map_gdf.columns:
                 tooltip_fields.append(balance_variable)
                 tooltip_aliases.append(f'{balance_variable.replace("_", " ").title()}:') # Nicer alias

            tooltip_fields = [f for f in tooltip_fields if f in map_gdf.columns]
            tooltip_aliases = tooltip_aliases[:len(tooltip_fields)]

            unique_territories = sorted(map_gdf['territory'].unique())
            num_territories = len(unique_territories)
            colors = generate_colormap(num_territories)
            territory_color_map = dict(zip(unique_territories, colors))

            folium.GeoJson(
                map_gdf,
                style_function=lambda feature: {
                    'fillColor': territory_color_map.get(feature['properties']['territory'], '#808080'),
                    'color': 'black',
                    'weight': 0.5,
                    'fillOpacity': 0.6,
                },
                highlight_function=lambda x: {'weight': 2, 'color': 'yellow', 'fillOpacity': 0.75},
                tooltip=folium.features.GeoJsonTooltip(
                    fields=tooltip_fields,
                    aliases=tooltip_aliases,
                    localize=True, sticky=False, labels=True,
                    style="background-color: white; color: #333333; font-family: arial; font-size: 12px; padding: 10px;"
                ),
                name='Territories'
            ).add_to(m)

            folium.LayerControl().add_to(m)
            m.fit_bounds(map_gdf.get_bounds())

            print("Folium map generated.")
            st.success("Map generated successfully.")
            return m
        except Exception as map_e:
            st.error(f"An error occurred during map generation: {map_e}")
            print(f"Map generation error: {map_e}")
            return None

@st.cache_data
def generate_summary_stats(_gdf):
    """Generates summary statistics dictionary from the final GeoDataFrame."""
    if _gdf is None or _gdf.empty:
        return {}
    try:
        stats = {}
        stats['total_zctas'] = len(_gdf)
        stats['total_hcps'] = int(_gdf['hcp_count'].sum()) if 'hcp_count' in _gdf.columns else 'N/A'
        stats['num_segments'] = _gdf['segment'].nunique() if 'segment' in _gdf.columns else 'N/A'
        stats['num_territories'] = _gdf['territory'].nunique() if 'territory' in _gdf.columns else 'N/A'

        if 'segment' in _gdf.columns:
            seg_summary_cols =
            valid_seg_summary_cols = [c for c in seg_summary_cols if c in _gdf.columns]
            stats['segment_summary'] = _gdf.groupby('segment')[valid_seg_summary_cols].agg(
                num_zctas=('ZCTA', 'count'),
                total_hcps=('hcp_count', 'sum'),
                avg_trx=('total_trx_volume', 'mean')
            ).reset_index().to_dict('records')
        else:
            stats['segment_summary'] =

        if 'territory' in _gdf.columns:
            terr_summary_cols =
            valid_terr_summary_cols = [c for c in terr_summary_cols if c in _gdf.columns]
            stats['territory_summary'] = _gdf.groupby('territory')[valid_terr_summary_cols].agg(
                num_zctas=('ZCTA', 'count'),
                total_hcps=('hcp_count', 'sum'),
                total_trx=('total_trx_volume', 'sum'),
                avg_trx_per_zcta=('total_trx_volume', 'mean')
            ).reset_index().to_dict('records')
        else:
             stats['territory_summary'] =

        return stats
    except Exception as e:
        st.error(f"Error generating summary stats: {e}")
        print(f"Error generating summary stats: {e}")
        return {"error": f"Could not generate summary stats: {e}"}

@st.cache_data
def convert_df_to_csv(df):
    """Converts DataFrame/GeoDataFrame to CSV bytes, dropping geometry."""
    df_export = df.copy()
    if 'geometry' in df_export.columns:
        df_export = df_export.drop(columns='geometry')
    # Convert potentially problematic types (like lists if aggregations created them) to strings
    for col in df_export.select_dtypes(include=['object']).columns:
         if df_export[col].apply(lambda x: isinstance(x, list)).any():
              df_export[col] = df_export[col].astype(str)
    return df_export.to_csv(index=False).encode('utf-8')

# --- Streamlit App Layout ---

st.set_page_config(layout="wide", page_title="HCP Segmentation & Territory Optimization")

st.title("âš•ï¸ HCP Segmentation & Territory Optimization Tool (Streamlit)")
st.markdown("""
Upload HCP data (CSV) and ZCTA boundaries (Shapefile in ZIP) to perform
segmentation and territory optimization. Configure parameters in the sidebar.
""")

# --- Initialize Session State ---
# Use session state to store dataframes and results across reruns
default_values = {
    'hcp_data': None,             # Raw loaded HCP data (DataFrame or GeoDataFrame)
    'zcta_gdf': None,             # Raw loaded ZCTA data (GeoDataFrame)
    'processed_gdf': None,        # ZCTA data after preprocessing & aggregation
    'segmented_gdf': None,        # ZCTA data after segmentation
    'territory_gdf': None,        # Final ZCTA data after optimization
    'weights': None,              # Calculated spatial weights
    'data_loaded': False,
    'segmentation_run': False,
    'optimization_run': False,
    'selected_segment_features':, # Store user selections
    'selected_n_clusters': 5,
    'selected_balance_var': None,
    'selected_n_territories': 10,
    'selected_min_territory_size': 1
}
for key, value in default_values.items():
    if key not in st.session_state:
        st.session_state[key] = value

# --- Sidebar Controls ---
with st.sidebar:
    st.header("1. Inputs")
    hcp_file = st.file_uploader(
        "Upload HCP Data (CSV)",
        type="csv",
        key="hcp_uploader_widget",
        help=f"CSV must contain: {HCP_ID_COL}, {HCP_LAT_COL}, {HCP_LON_COL}, {HCP_TRX_COL}. Optional: {HCP_ZIP_COL}."
    )
    zcta_file = st.file_uploader(
        "Upload ZCTA Boundaries (ZIP)",
        type="zip",
        key="zcta_uploader_widget",
        help=f"ZIP must contain.shp,.shx,.dbf. Shapefile needs ZCTA ID (e.g., {ZCTA_COL_PRIMARY})."
    )

    # Load Data Button
    if st.button("Load and Process Data", key="load_data_button", disabled=not (hcp_file and zcta_file)):
        if hcp_file and zcta_file:
            # Read file bytes
            hcp_bytes = hcp_file.getvalue()
            zcta_bytes = zcta_file.getvalue()

            # Reset state before loading new data
            for key in default_values: st.session_state[key] = default_values[key]

            hcp_data_loaded, zcta_data_loaded = load_data(hcp_bytes, zcta_bytes)
            if hcp_data_loaded is not None and zcta_data_loaded is not None:
                st.session_state.hcp_data = hcp_data_loaded
                st.session_state.zcta_gdf = zcta_data_loaded
                st.session_state.data_loaded = True
                st.success("Data loaded successfully! Preprocessing...")
                # Trigger preprocessing immediately after loading
                st.session_state.processed_gdf = preprocess_and_aggregate(
                    st.session_state.hcp_data,
                    st.session_state.zcta_gdf
                )
                if st.session_state.processed_gdf is None:
                     st.error("Preprocessing failed after loading. Check data compatibility.")
                     st.session_state.data_loaded = False # Reset flag if preprocessing fails
                else:
                     st.success("Preprocessing complete. Configure analysis below.")
                st.rerun() # Update UI state
            else:
                st.error("Data loading failed. Check error messages above.")
                st.session_state.data_loaded = False
        else:
            st.warning("Please upload both files.")

    st.divider()

    # --- Analysis Forms (conditional on data processing) ---
    if st.session_state.data_loaded and st.session_state.processed_gdf is not None:
        st.header("2. Configure & Run")

        # --- Segmentation Form ---
        with st.form("segmentation_form"):
            st.subheader("K-Means Segmentation (on ZCTAs)")
            processed_gdf = st.session_state.processed_gdf
            # Use scaled features generated during preprocessing
            potential_features = [col for col in processed_gdf.columns if col.endswith('_scaled')]

            if not potential_features:
                st.warning("No scaled features found in processed data for segmentation.")
                segment_features =
            else:
                 # Use session state value if available, otherwise default
                 current_selection = st.session_state.selected_segment_features if st.session_state.selected_segment_features else potential_features

                 segment_features = st.multiselect(
                     "Select Scaled Features for Segmentation",
                     options=potential_features,
                     default=current_selection,
                     key="segment_features_select_widget",
                     help="Select scaled ZCTA features (e.g., hcp_count_scaled, total_trx_volume_scaled)."
                 )

            n_clusters = st.number_input(
                "Number of Segments (K)",
                min_value=2, max_value=min(50, len(processed_gdf)-1 if len(processed_gdf)>1 else 2), # Ensure K < N
                value=st.session_state.selected_n_clusters, step=1,
                key="n_clusters_input_widget"
            )

            segment_submitted = st.form_submit_button("Run Segmentation")

            if segment_submitted:
                if not segment_features:
                     st.warning("Please select at least one scaled feature for segmentation.")
                else:
                    # Store selections
                    st.session_state.selected_segment_features = segment_features
                    st.session_state.selected_n_clusters = n_clusters

                    # Run segmentation
                    segmented_result = segment_zctas(
                        st.session_state.processed_gdf,
                        st.session_state.selected_segment_features,
                        st.session_state.selected_n_clusters
                    )
                    if segmented_result is not None:
                        st.session_state.segmented_gdf = segmented_result
                        st.session_state.segmentation_run = True
                        # Reset optimization if segmentation is re-run
                        st.session_state.territory_gdf = None
                        st.session_state.optimization_run = False
                        st.success("Segmentation successful!") # Show success inside form
                        # No rerun needed here, state update will trigger main panel update
                    else:
                         st.error("Segmentation failed.") # Show error inside form

        # --- Optimization Form (conditional on segmentation) ---
        if st.session_state.segmentation_run and st.session_state.segmented_gdf is not None:
            with st.form("optimization_form"):
                st.subheader("SKATER Territory Optimization")
                segmented_gdf = st.session_state.segmented_gdf
                potential_balance_vars = [col for col in segmented_gdf.columns if col.endswith('_scaled')]

                if not potential_balance_vars:
                     st.warning("No scaled features available for balancing.")
                     balance_variable = None
                else:
                    # Default to first scaled feature if none selected previously
                    current_balance_var = st.session_state.selected_balance_var if st.session_state.selected_balance_var and st.session_state.selected_balance_var in potential_balance_vars else potential_balance_vars

                    balance_variable = st.selectbox(
                        "Select Scaled Variable to Balance Territories On",
                        options=potential_balance_vars,
                        index=potential_balance_vars.index(current_balance_var) if current_balance_var and current_balance_var in potential_balance_vars else 0,
                        key="balance_var_select_widget",
                        help="SKATER uses this variable to assess homogeneity within territories."
                    )

                n_territories = st.number_input(
                    "Number of Territories",
                    min_value=2, max_value=min(100, len(segmented_gdf)-1 if len(segmented_gdf)>1 else 2),
                    value=st.session_state.selected_n_territories, step=1,
                    key="n_territories_input_widget"
                )
                min_territory_size = st.number_input(
                    "Minimum ZCTAs per Territory (Floor)",
                    min_value=1, value=st.session_state.selected_min_territory_size, step=1,
                    key="min_territory_size_input_widget"
                )

                optimize_submitted = st.form_submit_button("Optimize Territories")

                if optimize_submitted:
                    if not balance_variable:
                        st.warning("Please select a scaled variable to balance territories on.")
                    else:
                        # Store selections
                        st.session_state.selected_balance_var = balance_variable
                        st.session_state.selected_n_territories = n_territories
                        st.session_state.selected_min_territory_size = min_territory_size

                        # Calculate weights (cached)
                        weights, cleaned_gdf = calculate_spatial_weights(st.session_state.segmented_gdf)
                        st.session_state.weights = weights
                        # Use the potentially cleaned GDF from weights calculation
                        gdf_for_opt = cleaned_gdf if weights is not None else st.session_state.segmented_gdf

                        if st.session_state.weights:
                            territory_result = optimize_territories(
                                gdf_for_opt, # Use potentially cleaned GDF
                                st.session_state.weights,
                                [st.session_state.selected_balance_var], # SKATER expects a list
                                st.session_state.selected_n_territories,
                                st.session_state.selected_min_territory_size
                            )
                            if territory_result is not None:
                                st.session_state.territory_gdf = territory_result
                                st.session_state.optimization_run = True
                                st.success("Territory optimization successful!")
                                # No rerun needed here
                            else:
                                st.error("Territory optimization failed.")
                        else:
                             st.error("Could not calculate spatial weights. Optimization aborted.")

    elif st.session_state.data_loaded:
         st.info("Preprocessing failed or data incompatible. Cannot configure analysis.")
    else:
         st.info("Upload and process data using the buttons above to configure analysis.")


# --- Main Panel Display ---

if not st.session_state.data_loaded:
    st.info("ðŸ‘‹ Welcome! Please upload HCP (CSV) and ZCTA boundary (ZIP) files using the sidebar, then click 'Load and Process Data'.")
elif not st.session_state.segmentation_run:
    st.info("Data loaded and preprocessed. Please run Segmentation using the sidebar controls.")
elif not st.session_state.optimization_run:
    st.info("Segmentation complete. Please run Territory Optimization using the sidebar controls.")
else:
    # Data loaded and optimization complete, display results in tabs
    st.success("Analysis Complete! View results below.")
    tab_map, tab_segmentation, tab_territory = st.tabs()

    # --- Map Tab ---
    with tab_map:
        st.header("Map Visualization")
        if st.session_state.territory_gdf is not None:
            st.markdown("Visualize the optimized sales territories. Hover over a ZCTA for details.")
            balance_var_used = st.session_state.selected_balance_var # Get balance var used

            # Create the map object (cached)
            map_object = create_map(
                st.session_state.territory_gdf,
                balance_var_used
            )
            if map_object:
                # Display the map using st_folium
                st_folium(map_object, width='100%', height=600, returned_objects=)
            else:
                st.error("Failed to generate map.")
        else:
            st.warning("Territory optimization results not available for map display.")

    # --- Segmentation Data Tab ---
    with tab_segmentation:
        st.header("Segmentation Results (K-Means on ZCTAs)")
        if st.session_state.segmented_gdf is not None:
            segmented_data = st.session_state.segmented_gdf
            features_used = st.session_state.selected_segment_features
            n_clusters = st.session_state.selected_n_clusters

            st.markdown(f"ZCTAs segmented into **{n_clusters}** groups using K-Means based on features: **{', '.join(features_used)}**.")

            # Generate summary stats (cached)
            summary_stats = generate_summary_stats(segmented_data) # Use segmented data for this tab

            col1, col2 = st.columns(2)
            with col1:
                st.metric("Total Segments", summary_stats.get('num_segments', 'N/A'))
                if summary_stats.get('segment_summary'):
                    segment_counts_df = pd.DataFrame(summary_stats['segment_summary']).set_index('segment')['num_zctas']
                    st.write("ZCTAs per Segment:")
                    st.bar_chart(segment_counts_df)
                else:
                     st.write("No segment summary data.")

            with col2:
                 st.metric("Total ZCTAs Segmented", summary_stats.get('total_zctas', 'N/A'))
                 st.metric("Total HCPs in Segmented ZCTAs", f"{summary_stats.get('total_hcps', 'N/A'):,}")


            st.subheader("Segment Characteristics (ZCTA Averages)")
            if summary_stats.get('segment_summary'):
                 segment_summary_df = pd.DataFrame(summary_stats['segment_summary'])
                 st.dataframe(segment_summary_df.style.format({"avg_trx": "{:,.2f}", "total_hcps": "{:,}", "num_zctas": "{:,}"}))
            else:
                 st.info("No segment summary data available.")

            st.subheader("Segmented ZCTA Data Preview (Top 5 Rows)")
            display_df = segmented_data.drop(columns='geometry', errors='ignore')
            st.dataframe(display_df.head())

            # Download Button for Segmented ZCTA Data
            try:
                csv_segmented_zcta = convert_df_to_csv(segmented_data)
                st.download_button(
                    label="Download Segmented ZCTA Data (CSV)",
                    data=csv_segmented_zcta,
                    file_name="segmented_zcta_data.csv",
                    mime="text/csv",
                    key="download_seg_zcta"
                )
            except Exception as e:
                st.error(f"Error preparing segmented ZCTA data for download: {e}")
        else:
            st.info("Segmentation has not been run yet or failed.")

    # --- Territory Data Tab ---
    with tab_territory:
        st.header("Territory Optimization Results (SKATER)")
        if st.session_state.territory_gdf is not None:
            territory_data = st.session_state.territory_gdf
            balance_var_used = st.session_state.selected_balance_var
            n_territories = st.session_state.selected_n_territories

            st.markdown(f"ZCTAs optimized into **{territory_data['territory'].nunique()}** territories using SKATER, balancing on: **{balance_var_used}** (Target: {n_territories}).")

            # Generate summary stats (cached) - use final territory data
            summary_stats = generate_summary_stats(territory_data)

            col1, col2 = st.columns(2)
            with col1:
                st.metric("Total Territories Created", summary_stats.get('num_territories', 'N/A'))
                if summary_stats.get('territory_summary'):
                    territory_counts_df = pd.DataFrame(summary_stats['territory_summary']).set_index('territory')['num_zctas']
                    st.write("ZCTAs per Territory:")
                    st.bar_chart(territory_counts_df)
                else:
                     st.write("No territory summary data.")
            with col2:
                 st.metric("Total ZCTAs Optimized", summary_stats.get('total_zctas', 'N/A'))
                 st.metric("Total HCPs in Optimized ZCTAs", f"{summary_stats.get('total_hcps', 'N/A'):,}")


            st.subheader("Territory Balance Analysis")
            if summary_stats.get('territory_summary'):
                 territory_summary_df = pd.DataFrame(summary_stats['territory_summary'])
                 st.dataframe(territory_summary_df.style.format({
                     "total_hcps": "{:,}",
                     "total_trx": "{:,.2f}",
                     "avg_trx_per_zcta": "{:,.2f}",
                     "num_zctas": "{:,}"
                 }))
                 # Visualize balance for the selected variable
                 if balance_var_used and balance_var_used.replace('_scaled','') in territory_data.columns:
                      balance_col_orig = balance_var_used.replace('_scaled','')
                      st.write(f"Sum of '{balance_col_orig}' per Territory:")
                      balance_chart_data = territory_data.groupby('territory')[balance_col_orig].sum()
                      st.bar_chart(balance_chart_data)
                 elif 'hcp_count' in territory_data.columns: # Fallback visualization
                      st.write(f"Sum of 'hcp_count' per Territory:")
                      balance_chart_data = territory_data.groupby('territory')['hcp_count'].sum()
                      st.bar_chart(balance_chart_data)

            else:
                 st.info("No territory summary data available.")


            st.subheader("Optimized Territory Data Preview (Top 5 Rows)")
            display_df = territory_data.drop(columns='geometry', errors='ignore')
            st.dataframe(display_df.head())

            # Download Button for Optimized Territory Data
            try:
                csv_territory = convert_df_to_csv(territory_data)
                st.download_button(
                    label="Download Optimized Territory Data (CSV)",
                    data=csv_territory,
                    file_name="optimized_territory_data.csv",
                    mime="text/csv",
                    key="download_terr_data"
                )
            except Exception as e:
                st.error(f"Error preparing territory data for download: {e}")
        else:
            st.info("Territory Optimization has not been run yet or failed.")

# --- Footer ---
st.sidebar.markdown("---")
st.sidebar.info("Streamlit POC for HCP Segmentation & Territory Optimization.")